#Problem 1

import numpy as np
import pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score
import matplotlib.pyplot as plt

data = load_breast_cancer()
X = data.data
y = data.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

max_k = 10 
kernels = ['linear', 'poly', 'rbf', 'sigmoid']  # Kernel types to explore
results = {kernel: {'accuracy': [], 'precision': [], 'recall': []} for kernel in kernels}

for k in range(1, max_k + 1):
    #Apply PCA
    pca = PCA(n_components=k)
    X_train_pca = pca.fit_transform(X_train)
    X_test_pca = pca.transform(X_test)
    
    for kernel in kernels:
        #Train SVM with specified kernel
        svm = SVC(kernel=kernel, random_state=42)
        svm.fit(X_train_pca, y_train)
        
        #Make predictions
        y_pred = svm.predict(X_test_pca)
        
        #Calculate metrics
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred)
        recall = recall_score(y_test, y_pred)
        
        #Results
        results[kernel]['accuracy'].append(accuracy)
        results[kernel]['precision'].append(precision)
        results[kernel]['recall'].append(recall)

def plot_metrics(results, metric):
    plt.figure(figsize=(12, 8))
    for kernel in kernels:
        plt.plot(range(1, max_k + 1), results[kernel][metric], marker='o', label=f'{kernel} kernel')
    plt.xlabel('Number of Principal Components (K)')
    plt.ylabel(metric.capitalize())
    plt.title(f'{metric.capitalize()} vs Number of Principal Components')
    plt.legend()
    plt.grid()
    plt.show()

plot_metrics(results, 'accuracy')
plot_metrics(results, 'precision')
plot_metrics(results, 'recall')

logistic_results = {'accuracy': 0.94, 'precision': 0.92, 'recall': 0.91} 

#Problem 2

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.svm import SVR
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

data = pd.read_csv("https://raw.githubusercontent.com/HamedTabkhi/Intro-to-ML/main/Dataset/Housing.csv")

features = ['area', 'bedrooms', 'bathrooms', 'stories', 'mainroad', 'guestroom', 
            'basement', 'hotwaterheating', 'airconditioning', 'parking', 'prefarea']
X = data[features]
y = data['price']

X = pd.get_dummies(X, drop_first=True)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

max_k = 10  
kernels = ['linear', 'poly', 'rbf', 'sigmoid']
results = {kernel: {'mse': [], 'r2': []} for kernel in kernels}

for k in range(1, max_k + 1):
    #Apply PCA
    pca = PCA(n_components=k)
    X_train_pca = pca.fit_transform(X_train)
    X_test_pca = pca.transform(X_test)
    
    for kernel in kernels:
        #SVR with specified kernel
        svr = SVR(kernel=kernel)
        svr.fit(X_train_pca, y_train)
        
        #Prediction
        y_pred = svr.predict(X_test_pca)
        
        #Calculate metrics
        mse = mean_squared_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)
        
        results[kernel]['mse'].append(mse)
        results[kernel]['r2'].append(r2)

def plot_metric(results, metric, ylabel):
    plt.figure(figsize=(10, 6))
    for kernel in kernels:
        plt.plot(range(1, max_k + 1), results[kernel][metric], marker='o', label=f'{kernel} kernel')
    plt.xlabel('Number of Principal Components (K)')
    plt.ylabel(ylabel)
    plt.title(f'{ylabel} vs Number of Principal Components')
    plt.legend()
    plt.grid()
    plt.show()

plot_metric(results, 'mse', 'Mean Squared Error')
plot_metric(results, 'r2', 'R^2 Score')

ridge = Ridge()
ridge.fit(X_train, y_train)
y_pred_ridge = ridge.predict(X_test)

#Calculate metrics for Ridge Regression
ridge_mse = mean_squared_error(y_test, y_pred_ridge)
ridge_r2 = r2_score(y_test, y_pred_ridge)

#Plotting Ridge Baseline
for metric, ylabel in zip(['mse', 'r2'], ['Mean Squared Error', 'R^2 Score']):
    plot_metric(results, metric, ylabel)
    plt.axhline(y=ridge_mse if metric == 'mse' else ridge_r2, color='r', linestyle='--', label='Ridge Regression')
    plt.legend()
    plt.show()
